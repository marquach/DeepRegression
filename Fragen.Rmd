---
title: "Fragen"
output: html_document
date: "2023-01-09"
---

## Fragen:

- Bei precalc_gam muss nicht verändert -> DR: Richtig

- Alles bis zu return_prepoc = T kann verwendet bzw. muss nicht verändert werden
  - scheinbar nicht so ganz vll doch nicht -> DR: Richtig, da verarbeitete Terme bereits auch das entsprechende Layer enthalten
  - \$layer problematisch! (wegen layer_args, layer_class, usw.) -> DR: Richtig
    - Sollte aber eigentlich passen wenn layer_generator torch nutzt -> DR: Richtig
    - (Überlegung: Sollte bei layer_class dann bei spline z.b. layer_spline_torch (siehe ..._functions dafür) stehen) 
    -> DR: eine Option: globale Variable in R's options() die angibt, ob TF oder torch, und layer_generator nimmt dann entsprechend die richtigen Funktionen
    -> DR: weitere Option: layer_spline_torch usw verwenden und dann die globale Option ob TF oder torch in process_terms abfragen
    
    
- Fragen zu deepregression:::process_terms:
  - create_P muss nicht verändert werden -> DR: Richtig
    - create_P ist eine diagonal matrix mit Smooth Parametern der Terme
    - Modul bekommt also seinen eigenen Input -> DR: Richtig
      - model_builder dann vll sowie bei NN_VerständnisSkript.R ganz am Ende -> DR: später reden
      - hat noch nicht mit luz funktioniert -> DR: später reden
      - Funktioniert jetzt auch mit luz. dataloader musste genutzt werden
      
- manche processor nutzen layer_generator (gam_processor) und andere nicht (multiply_processor) 
  -> DR: das ist aktuell noch eine Baustelle von deepreg, ja. Viele der "übrigen" processor passen leider nur nicht in das bisherige Konzept
  - hier als Input engine=torch um die Layer mit Torch zu initialisieren? -> DR: Zum Beispiel (entweder eigene torch processors oder alle bisherigen anpassen)
  - Sollte man anpassen, dass alle layer_generator() nutzen und dann layer_generator(..., engine = Torch)
  - layer_class nutzt dann bei gam_processor layer_spline_torch oder halt engine = Torch und dann nur die torch layer ziehen
  - Erstmal nur ein paar prozessoren hardcoden?

- makeInputs erstellt Keras Tensoren mit Shape(None,...) -> DR: Richtig, None ist die Batch shape
  - Bei Torch muss Shape immer gleich angegeben werden, oder? -> DR: Richtig
  - Hier leeren (torch_empty(Dimensionen)) oder direkt mit Daten (torch_tensor(...))
  -> DR: Evtl. nicht nötig. Keras braucht die Input Tensoren um das keras Model zu definieren. 
  Ein nn.Module in Pytorch braucht das nicht, sondern nur in_features (oder in_channels o.Ä.)
  - Input wird bei Torch nicht gebraucht. makeInputs wird also nicht benötigt bei Torch Ansatz
  
- Fragen subnetwork_init:
  - outputs in subnetwork_init wird auf shape(None, 1) bei allen reduziert. Warum? 
  -> DR: Jedes Subnetwork lernt einen skalaren Verteilungsparameter. Es gibt aber auch Ausnahmen
    - Da Ergebnis des NN pro Beobachtung ein skalar ist
  - summary_layer per default layer_add_identity
    - Torch hat m.M.n. kein layers.add
    -> DR: torch.add sollte funktionieren
  - subnetwork_init noch nicht ganz klar
    - subnetwork_init baut tensoren (bis auf gampart) und layer
    - enthält tensoren für inputs und outputs
    - bei sturkturieten Parts nur dense layer bzw. nn_linear
    - summary_layer setzt diese dann zusammen (z.b. layer_add_identity)
    - return ist liste aus input[keep_inputs_in_return] und output
    -> DR: Richtig. Im Endeffekt definiert subnetwork_init je Verteilungsparameter ein Subnetzwerk
    im Keras Stil (d.h. wie komme ich von einem Input zum Output des Subnetzwrks)
    
- Netzwerk bei uns über nn_module() bzw. nn_sequential() in deepregression() -> DR: nn_module da nicht alle sequential sein werden

- model_builder bei uns?   -> DR: selbst definierte Funktion (u.A. mit setup())
  - deepreg. nutzt keras_dr (bzw. keras_model)
  - luz mit setup()? -> DR: Richtig. Idealerweise müsste man nur model_fun in model_builder ersetzen und den 
  Rest des codes in model_fun generischer schreiben (bspw. den compile Schritt in eine Funktion model_compile
  die dann im Fall von torch eben genau setup() wäre)
  
- Einfachste Fälle überlegen für Torch 
  - (orthogonalize = F) immer nutzen
  - Intercept Only
  - nur lineares Model
  - spline
  - nur deep_model
  -> DR: Klingt gut, aber für einen Prototypen / im Rahmen des Kurses reicht auch weniger
  
- Nächste Schritte?
  - makeInputs() anpassen
    - brauchen hier dann alle Dimensionen, da shape(None, ...) bei Torch nicht geht
    - bei den procs die input_dim zu vektor machen, da kann mit (torch_empty(Dimensionen)) gearbeitet werden
  - layer_generator direkt ändern?
  - oder erstmal ..._processor von einfachsten Fällen anpassen und mit deepregression testen
    - int_processor
    - lin_processor
    - gam_processor
  -> DR: Später reden. 

  


  
